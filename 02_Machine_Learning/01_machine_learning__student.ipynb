{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sJGFf3ZWK8OV"
      },
      "source": [
        "# Data Science for the Automotive Industry - 1st and 2nd practical session - ML\n",
        "\n",
        "In this session, we will dive into an example of classic machine learning models, trying to extract the most out of a simple and small dataset using simple but powerful methods.\n",
        "\n",
        "We will explore a data set with car sales of different brands and models in USA. We will follow the order below:\n",
        "1. Import of required libraries\n",
        "2. Loading the dataset from google drive\n",
        "3. Exploratory data analysis\n",
        "4. Unsupervised learning\n",
        "5. Supervised learning \n",
        "6. Conclusions and take aways\n",
        "\n",
        "<!-- This data has been downloaded from [kaggle.com](https://www.kaggle.com/) and it can be found using [this link](https://www.kaggle.com/gagandeep16/car-sales) for car_sales,[this link](https://www.kaggle.com/smritisingh1997/car-salescsv) for car_sales_2 and [this link](https://www.kaggle.com/sachinsachin/car-sales?select=Car+Sales.xlsx) for car_sales_3. -->\n",
        "\n",
        "Developed by Nicolas Gutierrez.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN6Kl4JPM5jo"
      },
      "source": [
        "## 1 - Importing required libraries\n",
        "It is a good practice loading the required libraries for the code at the start of it. Additionally, doing it this way you can have some hints about what the code below will do, just by checking the types of libraries imported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2p3XlJUtR5p"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# Files\n",
        "import glob\n",
        "# Data loading and manipulation\n",
        "import pandas as pd\n",
        "# Numeric operations\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics, svm, tree\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.cluster import KMeans\n",
        "# Representation\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from matplotlib.gridspec import GridSpec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki-O96BrMk2p"
      },
      "source": [
        "## 2 - Loading the dataset from Google Drive\n",
        "The best way to work with a dataset from google colab is loading it from the same folder where the notebook is stored using the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgH1gPJcsxAG"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCfIqXCGNRiv"
      },
      "source": [
        "Once google drive is mounted and access is granted, we can use glob library to check the directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERC_wBNQamGm"
      },
      "source": [
        "#### Ex 1: Locate the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl-MNzcNtUbj"
      },
      "outputs": [],
      "source": [
        "# Modify/Complete the following line\n",
        "list_of_files = glob.glob('/content/drive/MyDrive/*')\n",
        "#\n",
        "\n",
        "print(list_of_files)\n",
        "## The result of this print should be 3 paths ending in */car_sales_3.xlsx, */car_sales_2.csv, */car_sales.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdoBA7LUNj2N"
      },
      "source": [
        "From the previous list, we can use Pandas and load the csv \"Car_sales\" into memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKvCgz3kavKV"
      },
      "source": [
        "#### Ex 2: Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwHzFJ5at9FR"
      },
      "outputs": [],
      "source": [
        "#### Exercise 2: Load the data from the file car_sales.csv using pandas, find the order to load a csv with pandas\n",
        "\n",
        "# Modify the following line\n",
        "car_sales = \n",
        "#\n",
        "\n",
        "print(f\"The number of rows of the file are: {len(car_sales)}\")\n",
        "## The result should be 157"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G8563pYNy9Y"
      },
      "source": [
        "## 3 - Exploratory Data Analysis\n",
        "In this section, we will check the data to see what features/models/brands ... are included. Additionally we will check the distribution of values and get a feeling of statisical parameters, extremes and relationship of variables. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YERD1sx3eBPX"
      },
      "source": [
        "### DataFrame description\n",
        "Pandas has several convenient functions for describing the data in the DataFrame in a very simple way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE2b6NK3bVTa"
      },
      "source": [
        "#### Ex 3: Check first rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QobdVUOENzaC"
      },
      "outputs": [],
      "source": [
        "### Exercise 3: Find a way of showing the first or last rows of a pandas dataframe\n",
        "\n",
        "# Print 5 first values of the pandas dataframe\n",
        "\n",
        "#\n",
        "\n",
        "# * Curb_weight is the weight of the vehicle including a full tank of fuel and all standard equipment\n",
        "## The result should be a table in which the first column is Manufacturer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGCWwKMrbcl_"
      },
      "source": [
        "#### Ex 4: Show a summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4dRir63yyPw"
      },
      "outputs": [],
      "source": [
        "### Exercise 4: Find a method in your data frame car_sales that shows you a summary or a \n",
        "###   description of the content of the columns of the dataframe, Use the option of the \n",
        "###   method to describe all the data\n",
        "\n",
        "# Insert the order here\n",
        "\n",
        "#\n",
        "\n",
        "## The result should be a table with Manufacturer in the first column and then a some handy statistical values in the rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM0w5ZZ3bg8H"
      },
      "source": [
        "#### Ex 5: Describe the data further"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flBNw9hYvZlq"
      },
      "outputs": [],
      "source": [
        "### Exercise 5: Let's describe a bit more the data we have\n",
        "\n",
        "# Check the size of the data set as how many rows and columns it has\n",
        "print(f\"The dataset has {} rows and {} columns\\n\")\n",
        "\n",
        "# Check how many and which ones are the columns of the dataset\n",
        "print(f\"Column names are {}\\n\")\n",
        "\n",
        "# How many different brands and models are included in the study\n",
        "print(f\"{len()} different brands and {len()} models are considered\\n\")\n",
        "\n",
        "# How many types of car\n",
        "print(f\"Following vehicle types are included: {}\\n\")\n",
        "\n",
        "# What is the amount of car sales according to this dataset? Are those values sensible? How would you know?\n",
        "print(f\"Total amount of sales: {}\\n\")\n",
        "\n",
        "# What is the most succesful brand of all and what is the market share of this brand?\n",
        "print(f\"Most succesful brand (by sales): {}, \"\n",
        "f\"with {}% of the sales\")\n",
        "\n",
        "# What variables do you see as possible \"inputs\" and \"outputs\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSApT1XqeKE7"
      },
      "source": [
        "### Bar and distribution plots\n",
        "Pandas allows plotting directly from the library without going to any specific library. This feature is very convenient when producing simple plots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUFafVwvbrVG"
      },
      "source": [
        "#### Ex 6: Bar plot by Manufacturer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsd4l_V4x5Yh"
      },
      "outputs": [],
      "source": [
        "### Exercise 6: Plot the car sales of every manufacturer so we can have a view of the most successful manufacturers\n",
        "\n",
        "# Don't use matplotlib for this, but directly from pandas, this should be done in one line\n",
        "car_sales.\n",
        "#\n",
        "\n",
        "plt.title(\"Sales by manufacturer\")\n",
        "plt.ylabel(\"Sales [1000s of units]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuQJw5AAbygH"
      },
      "source": [
        "#### Ex 7: Bar plot by Sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89fZ9Wi7s3yy"
      },
      "outputs": [],
      "source": [
        "### Exercise 7: Plot the car sales, but now based on the models.\n",
        "\n",
        "# Use the option figsize=(20, 5) to stretch the graph and see it better\n",
        "car_sales\n",
        "#\n",
        "\n",
        "plt.title(\"Sales by Model\")\n",
        "plt.ylabel(\"Sales [1000s of units]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P94FXY4-b1Y3"
      },
      "source": [
        "#### Ex 8: Plot Distribution of relevant variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt_Heog72mWl"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# Let's plot histograms of all variables as this will help us understand how the values are distributed\n",
        "def plot_histograms(vars, xlabels, title=None):\n",
        "  ncols = len(vars)\n",
        "  fig, ax = plt.subplots(ncols=ncols)\n",
        "  plt.ylabel('Frequency [-]')\n",
        "  if title:\n",
        "    plt.suptitle(title)\n",
        "  for i in range(len(vars)):\n",
        "    average = car_sales[vars[i]].mean()\n",
        "    median_value = car_sales[vars[i]].median()\n",
        "    car_sales[vars[i]].plot.hist(ax=ax[i], sharey=True, figsize=(18,5))\n",
        "    ax[i].set_title(\"Distribution of \\n\" + vars[i])\n",
        "    ax[i].set_xlabel(xlabels[i])\n",
        "    ax[i].axvline(x=average, color='red', zorder=1)\n",
        "    ax[i].axvline(x=median_value, color='black', zorder=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPK0yOXUZzZI"
      },
      "outputs": [],
      "source": [
        "### Exercise 8: Use the function above to plot the 4 most relevant variables\n",
        "\n",
        "# Modify the following lines\n",
        "variables_to_plot = ['', '', '', '']\n",
        "xlabel = ['', '', '', '']\n",
        "title = \"\"\n",
        "plot_histograms(variables_to_plot, xlabel, title)\n",
        "#\n",
        "\n",
        "### What do you thing are the most relevant variables? \n",
        "### What do the red and black lines mean? \n",
        "### What can you extract from them being close together or separated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K3gYp7bb-G0"
      },
      "source": [
        "#### Ex 9: Plot Physical characteristics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmQuy4_etVkF"
      },
      "outputs": [],
      "source": [
        "### Exercise 9: Use plot_histogram function to plot 'Physical characteristics of the cars\n",
        "\n",
        "# Include your lines here\n",
        "variables_to_plot = ['', '', '', '']\n",
        "xlabel = [\"\", \"\", \"\", \"\"]\n",
        "title = \"\"\n",
        "plot_histograms(variables_to_plot, xlabel, title)\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvUFeGnotJqu"
      },
      "source": [
        "### Feature Engineering\n",
        "We will see a toy example of feature engineering.\n",
        "\n",
        "New features can be created as a combination of any other if required to enrich the model. For example in this data set we have width and length. Do you believe having the size will make a difference?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXOmrxAscFLI"
      },
      "source": [
        "#### Ex 10: Create new features (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVyCYfXxtPEf"
      },
      "outputs": [],
      "source": [
        "### Exercise 10: Feature engineering\n",
        "\n",
        "# Create a column name called Size as a relevant combination of Width and Length\n",
        "car_sales['Size'] = \n",
        "#\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjcV36I6eU_B"
      },
      "source": [
        "### Correlation matrix\n",
        "__Correlation__ is a statistical measure that indicates the extent to which two or more variables fluctuate in relation to each other. Correlation values are contained between -1 and 1. __The higher the absolute value of the correlation the higher the relationship between the variables__. \n",
        "Correlation is also the square root of the r2 accuracy coefficient from linear regressions.\n",
        "\n",
        "The correlation Matrix should always be one of the first steps in a machine learning project as allows checking easily which variables are related and which are not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBIxk6lScQsL"
      },
      "source": [
        "#### Ex 11: Correlation matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkhecG0n8jsA"
      },
      "outputs": [],
      "source": [
        "### Exercise 11: Pearson correlation matrix\n",
        "\n",
        "# Calculated the corr_matrix in the following line\n",
        "corr_matrix = \n",
        "#\n",
        "\n",
        "matfig = plt.figure(figsize=2*np.array([6.4, 4.8]))\n",
        "plt.matshow(np.abs(corr_matrix), cmap=cm.RdYlGn, fignum=matfig.number)\n",
        "plt.xticks(np.arange(0, len(corr_matrix.columns)), corr_matrix.columns.to_list(), rotation=90)\n",
        "plt.yticks(np.arange(0, len(corr_matrix.columns)), corr_matrix.columns.to_list())\n",
        "plt.colorbar()\n",
        "\n",
        "## Check the output of this cell, is there anything that calls your attention?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt4T57J1dlni"
      },
      "source": [
        "## 4 - Unsupervised learning\n",
        "In this section we will try to check if we can find any hidden relationship in the data by means of clustering. We will use [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and the famous [elbow method](https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/) to detect the optimal number of clusters.\n",
        "\n",
        "We will try to look for an example like [this](https://datatofish.com/k-means-clustering-python/), where clusters are clearly defined. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q83HcZBd13u"
      },
      "source": [
        "### Plot pair of Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttz1q4q3cXIJ"
      },
      "source": [
        "#### Ex 12: Scatter of pair of variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSCyLyDeHzuu"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "def plot_couple_of_variables(dataframe, x_name, y_name):\n",
        "  x_variable = dataframe[x_name]\n",
        "  y_variable = dataframe[y_name]\n",
        "\n",
        "  fig = plt.figure()\n",
        "  gs = GridSpec(4, 4)\n",
        "\n",
        "  ax_scatter = fig.add_subplot(gs[1:4, 0:3])\n",
        "  ax_hist_x = fig.add_subplot(gs[0,0:3])\n",
        "  ax_hist_y = fig.add_subplot(gs[1:4, 3])\n",
        "\n",
        "  ax_scatter.scatter(x_variable, y_variable)\n",
        "  ax_scatter.set_xlabel(x_name)\n",
        "  ax_scatter.set_ylabel(y_name)\n",
        "\n",
        "  ax_hist_x.hist(x_variable)\n",
        "  ax_hist_y.hist(y_variable, orientation = 'horizontal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9h5iJ3JHL1Pz"
      },
      "outputs": [],
      "source": [
        "### Exercise 12: Select two relevant variables from the car_sales dataset\n",
        "\n",
        "# Modify the following lines\n",
        "x_name = \"\"\n",
        "y_name = \"\"\n",
        "#\n",
        "\n",
        "plot_couple_of_variables(car_sales, x_name, y_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5sGIwiwelq6"
      },
      "source": [
        "### KMeans\n",
        "[KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) is a clustering method based on a iterative refinement technique  that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean ([read more](https://en.wikipedia.org/wiki/K-means_clustering#:~:text=k-means%20clustering%20is%20a,a%20prototype%20of%20the%20cluster.))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbKFzDnVceBd"
      },
      "source": [
        "#### Ex 13: Remove NaNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL0drj1xiRbh"
      },
      "outputs": [],
      "source": [
        "### Exercise 13: Prepare the dataset\n",
        "\n",
        "# First, remove the nan from the data set in the following line\n",
        "car_sales_nandropped = \n",
        "#\n",
        "\n",
        "X = np.array(list(zip(car_sales_nandropped[x_name], \n",
        "                      car_sales_nandropped[y_name])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjNxqUTCcjXp"
      },
      "source": [
        "#### Ex 14: Fit KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dbnn5rKOSNBh"
      },
      "outputs": [],
      "source": [
        "### Exercise 14: Fit a KMeans as a unsupervised learning method\n",
        "\n",
        "# Modify the number of clusters and check the results\n",
        "number_of_clusters = \n",
        "kmeanModel_fitted = \n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwu5EuAWcqdM"
      },
      "source": [
        "#### Ex 15: Use KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIeQxdrgvs-S"
      },
      "outputs": [],
      "source": [
        "### Exercise 15: Predict the data in X using kmeanModel object\n",
        "\n",
        "# Modify the following line\n",
        "results = kmeanModel_fitted.\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXOxoaL2S_jy"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "print(results)\n",
        "for i in range(number_of_clusters):\n",
        "  points_belonging_to_i_cluster = results == i\n",
        "  plt.scatter(car_sales_nandropped[x_name].to_numpy()[points_belonging_to_i_cluster], \n",
        "              car_sales_nandropped[y_name].to_numpy()[points_belonging_to_i_cluster], \n",
        "              label=f\"Cluster {i}\")\n",
        "plt.legend()\n",
        "plt.xlabel(x_name)\n",
        "plt.ylabel(y_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzQOpMtKeDJ3"
      },
      "source": [
        "### Elbow Method\n",
        "The number of clusters selected in the KMeans algorithm it is somehow a bit arbitrary and it might be a bit subjective sometimes. To avoid this issue, the Elbow method is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAML76oqeBOD"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "def elbow_method(X, x_name, y_name, x_variable, y_variable):\n",
        "  distortions = []\n",
        "  inertias = []\n",
        "  mapping1 = {}\n",
        "  mapping2 = {}\n",
        "  K = range(1, 10)\n",
        "  \n",
        "  for k in K:\n",
        "      # Building and fitting the model\n",
        "      kmeanModel = KMeans(n_clusters=k, n_init=10).fit(X)\n",
        "      kmeanModel.fit(X)\n",
        "  \n",
        "      distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n",
        "                                          'euclidean'), axis=1)) / X.shape[0])\n",
        "      inertias.append(kmeanModel.inertia_)\n",
        "  \n",
        "      mapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n",
        "                                    'euclidean'), axis=1)) / X.shape[0]\n",
        "      mapping2[k] = kmeanModel.inertia_\n",
        "    \n",
        "  plt.plot(K, distortions, 'bx-')\n",
        "  plt.xlabel('Values of K')\n",
        "  plt.ylabel('Distortion')\n",
        "  plt.title('The Elbow Method using Distortion')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wFekoH3fcIR"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "elbow_method(X, x_name, y_name, car_sales_nandropped[x_name], car_sales_nandropped[y_name])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trDKjnpnetgO"
      },
      "source": [
        "### PCA\n",
        "[PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) stands for __Principal Component Analysis__ and it is commonly used as a technique for dimensionality reduction. The PCA checks the principal components of the linear space defined by the data and establishes a transformation of the data to that space. The result is a \"compression\" of the data with loss of information depending on the number of variables. \n",
        "\n",
        "In this example we will use the PCA as a technique of clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF41um2OcyzJ"
      },
      "source": [
        "#### Ex 16: Preparation for PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "051aUI81iknm"
      },
      "outputs": [],
      "source": [
        "### Exercise 16: Preparation of the car_sales dataset for PCA\n",
        "\n",
        "# Drop the columns that contains strings from the car sales data set\n",
        "car_sales_string_dropped = car_sales.\n",
        "# Drop the columns that contain nans (Reset the index as well)\n",
        "car_sales_string_nandropped = car_sales_string_dropped\n",
        "#\n",
        "\n",
        "print(f\"Original data frame rows: {len(car_sales)}, new dataframe rows: {len(car_sales_string_nandropped)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZutlEV2Mc78c"
      },
      "source": [
        "#### Ex 17: Fitting PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "017tK6xYe4ql"
      },
      "outputs": [],
      "source": [
        "### Exercise 17: Fitting PCA\n",
        "\n",
        "# Instantiate a PCA model with 2 components (it is difficult plotting more than 2)\n",
        "pca_clustering = \n",
        "# Fit transform your PCA model into the car_sales_string_nandropped\n",
        "car_sales_pca = pca_clustering.\n",
        "#\n",
        "\n",
        "print(f\"The explained variance is {pca_clustering.explained_variance_}\")\n",
        "print(f\"The explained variance ratio is {pca_clustering.explained_variance_ratio_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeanrA-Dl2DZ"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "for i in range(car_sales_pca.shape[0]):\n",
        "  plt.scatter(car_sales_pca[i][0], \n",
        "              car_sales_pca[i][1], \n",
        "              marker = f\"${i}$\")\n",
        "plt.xlabel(\"PCA component 0\")\n",
        "plt.ylabel(\"PCA component 1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKNptwZcpNYk"
      },
      "source": [
        "What are the models that the PCA considers further from the average ones? Can you spot why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ILYviF6g1Kl"
      },
      "source": [
        "## 5I - Supervised Learning: Price prediction\n",
        "\n",
        "In this section we will try to create a model that help us understanding the behaviour of customers based on previous data.\n",
        "\n",
        "Wouldn't be cool if we could predict how much would someone pay for a car with certain characteristics?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s91-WfOikuW"
      },
      "source": [
        "### Features/labels selection\n",
        "Let's select first which ones will be the inputs (features) and outputs (labels) of our supervised learning problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PwDovc10vz6"
      },
      "source": [
        "#### Ex 18: Select variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EReY4jY9XPJ8"
      },
      "outputs": [],
      "source": [
        "### Exercise 18: Prepare the data to be used in a supervised learning method\n",
        "\n",
        "# Select the input and output variable names, input is a list, output is a single one\n",
        "input_variables = [\"\",\"\"]\n",
        "output_variable = ''\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFwJzjJSi8w5"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "input_variables.append(output_variable)\n",
        "\n",
        "X = car_sales[input_variables].dropna(axis=0)\n",
        "\n",
        "y = X[output_variable]\n",
        "X.drop([output_variable], axis=1, inplace=True)\n",
        "\n",
        "print(f\"{len(X.columns)} input variables: {X.columns}\")\n",
        "print(f\"Output variable is: {y.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxH8bbEskASu"
      },
      "source": [
        "### Normalization/Standarization of variables\n",
        "Data should be normalized or standarised before getting into any machine learning model. The purpose of this is to avoid biasing the model towards the higher magnitude variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS8jSQ2E00d-"
      },
      "source": [
        "#### Ex 19: Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3nX9jdDhga2"
      },
      "outputs": [],
      "source": [
        "### Exercise 19: Normalization\n",
        "\n",
        "# Find a scaler to normalise the data based on min and max (Have a look at sklearn.preprocessing)\n",
        "scaler_x = \n",
        "scaler_y = \n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZihZQcOy2Rc"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "scaler_x.fit(X)\n",
        "scaler_y.fit(y.to_numpy().reshape(-1,1))\n",
        "X_norm = scaler_x.transform(X)\n",
        "y_norm = scaler_y.transform(y.to_numpy().reshape(-1,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s6t-sIG03x0"
      },
      "source": [
        "#### Ex 20: Standarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QOGs6TGHlO0"
      },
      "outputs": [],
      "source": [
        "### Exercise 20: Standarization\n",
        "\n",
        "# Find a scaler to standarise the data based on average and standard deviation\n",
        "standard_x = \n",
        "standard_y = \n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8FDKHbAzHOu"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "standard_x.fit(X)\n",
        "standard_y.fit(y.to_numpy().reshape(-1,1))\n",
        "X_standard = standard_x.transform(X)\n",
        "y_standard = standard_y.transform(y.to_numpy().reshape(-1,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd4jpnNOkH9I"
      },
      "source": [
        "### Train/test split\n",
        "Next step we will do is plitting the data into train and test datasets. This is done to evaluate the models fairly. The models will be trained with the train data and they normally will perform very well on this one. The test dataset is used to evaluate the performance of the model on data it has never seen before or how good it generalises."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp8w08Ab1L7n"
      },
      "source": [
        "#### Ex 21: Train/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3yngabzeYee"
      },
      "outputs": [],
      "source": [
        "### Exercise 21: Split into Train/Test datasets\n",
        "\n",
        "# Loof for a function to split into train and test\n",
        "# Select Test size 0.3 and random state 1\n",
        "# (It is the same for every block)\n",
        "## Non preprocessed\n",
        "X_np_train, X_np_test, y_np_train, y_np_test = \\\n",
        "(X.to_numpy(), y.to_numpy().reshape(-1,1), ..., ...)\n",
        "\n",
        "## Normalization\n",
        "X_norm_train, X_norm_test, y_norm_train, y_norm_test = \\\n",
        "(X_norm, y_norm, ..., ...)\n",
        "\n",
        "## Standarization\n",
        "X_standard_train, X_standard_test, y_standard_train, y_standard_test = \\\n",
        "(X_standard, y_standard, ..., ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUe5yJE4pZka"
      },
      "source": [
        "### Fitting Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWopodH-12tL"
      },
      "source": [
        "#### Ex 22: Fit Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IRTsetjee--"
      },
      "outputs": [],
      "source": [
        "### Exercise 22: Find the routines to instantiate and fit the models indicated below\n",
        "\n",
        "def fit_models(features_train, labels_train):\n",
        "  # Linear Regression\n",
        "  lin_regress = \n",
        "  lin_regress.\n",
        "  # print(reg.coef_)\n",
        "  # print(reg.intercept_)\n",
        "\n",
        "  # Support vector machines\n",
        "  # Linear (Pay attention to the Linear detail)\n",
        "  svm_linear = \n",
        "  svm_linear.\n",
        "\n",
        "  # Poly (Pay attention to the Poly detail)\n",
        "  svm_poly = \n",
        "  svm_poly.\n",
        "\n",
        "  # RBF (Pay attention to the RBF detail)\n",
        "  svm_rbf = \n",
        "  svm_rbf.\n",
        "\n",
        "  # Decision tree regressor\n",
        "  clf = tree.DecisionTreeRegressor()\n",
        "  clf\n",
        "  return lin_regress, svm_linear, svm_poly, svm_rbf, clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxNSgE-gqBc9"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "features_train = X_np_train\n",
        "labels_train = y_np_train\n",
        "features_test = X_np_test\n",
        "labels_test = y_np_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bgc1xj09toCg"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "reg, svm_linear, svm_poly, svm_rbf, clf = fit_models(features_train, labels_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYdOAkqVrZD7"
      },
      "source": [
        "### Evaluation of the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3zUdF5HrBh3"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# Function for evaluating the methods\n",
        "def evaluation_of_methods(methods_vector, methods_name, \n",
        "                          features_train, features_test, \n",
        "                          labels_train, labels_test):\n",
        "  evaluation = pd.DataFrame(columns=['method', 'r2_train', 'r2_test', \n",
        "                                     'MSE_train', 'MSE_test', \n",
        "                                     'MAE_train', 'MAE_test'])\n",
        "  for i in range(len(methods_vector)):\n",
        "    print(f\"method name:{methods_name[i]}\")\n",
        "    results_dict = dict()\n",
        "    results_dict['method'] = methods_name[i]\n",
        "    \n",
        "    y_hat_train = methods_vector[i].predict(features_train)\n",
        "    y_hat_test = methods_vector[i].predict(features_test)\n",
        "\n",
        "    results_dict['r2_train'] = r2_score(labels_train, y_hat_train)\n",
        "    results_dict['r2_test'] = r2_score(labels_test, y_hat_test)\n",
        "\n",
        "    results_dict['MSE_train'] = mean_squared_error(labels_train, y_hat_train)\n",
        "    results_dict['MSE_test'] = mean_squared_error(labels_test, y_hat_test)\n",
        "\n",
        "    results_dict['MAE_train'] = mean_absolute_error(labels_train, y_hat_train)\n",
        "    results_dict['MAE_test'] = mean_absolute_error(labels_test, y_hat_test)\n",
        "\n",
        "    method_results = pd.DataFrame.from_dict([results_dict])\n",
        "    evaluation = pd.concat([evaluation,method_results], ignore_index = True)\n",
        "  return evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVqoyS3c2KOj"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# Function for plotting the results\n",
        "def plotting_methods(methods_vector, features_test, labels_test):\n",
        "  nrows = len(methods_vector)\n",
        "  fig, ax = plt.subplots(ncols=2, \n",
        "                        nrows=nrows, \n",
        "                        figsize=2.5*np.array([6.4, 4.8]), \n",
        "                        sharex='col')\n",
        "  for i in range(nrows):\n",
        "    # Prediction\n",
        "    y_hat = methods_vector[i].predict(features_test).ravel()\n",
        "    # Prediction vs real\n",
        "    ax[i, 0].scatter(labels_test, y_hat, label=\"Predictions\")\n",
        "    ax[i, 0].scatter(labels_test, labels_test, label=\"Real values\")\n",
        "    ax[i, 0].set_ylabel(f'Predictions {methods_name[i]}')\n",
        "    ax[i, 0].set_xlabel('Real values')\n",
        "    ax[i, 0].legend()\n",
        "    ax[i, 1].hist(labels_test.ravel()-y_hat)\n",
        "    ax[i, 1].set_xlabel('Error')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-iRqVudueLI"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "methods_vector = [reg, svm_linear, svm_poly, svm_rbf, clf]\n",
        "methods_name = ['Linear Regressor', 'SVM Linear', 'SVM Poly', 'SVM RBF', 'Decision Tree']\n",
        "evaluation = evaluation_of_methods(methods_vector, methods_name, \n",
        "                                   features_train, features_test,\n",
        "                                   labels_train, labels_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPgvUgpzsExZ"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYP5lHQoyT_Y"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "plotting_methods(methods_vector, features_test, labels_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZih-wwM-r7x"
      },
      "source": [
        "## 5II - Supervised Learning: Sales Prediction\n",
        "\n",
        "The marketing department of a new brand has contacted our group of data science with interest in sales prediction. Based on the dataset we have, we will try to create a model that can predict the number of sales of a model given its characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe74eJDDvnfu"
      },
      "source": [
        "### Features/labels preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrGJPHOV3jVU"
      },
      "source": [
        "#### Ex 23: One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5yDaS_P_kU_"
      },
      "outputs": [],
      "source": [
        "### Ex 23: One hot encoding of the Manufacturer\n",
        "\n",
        "# Look for a function in pandas to one-hot encode the manufacturer\n",
        "# TIP: It has to do with dummies\n",
        "manufacturer_one_hot = pd.(car_sales['Manufacturer'], dtype=float)\n",
        "#\n",
        "\n",
        "# Quick check:\n",
        "print(np.sum(manufacturer_one_hot.sum(axis=1)))\n",
        "print(len(manufacturer_one_hot))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjT5zCU2TfR5"
      },
      "outputs": [],
      "source": [
        "### Ex 24: Drop columns\n",
        "\n",
        "# Drop columns 'Manufacturer', 'Model', '__year_resale_value', 'Vehicle_type', 'Latest_Launch'\n",
        "car_sales_for_sales = car_sales.([], axis=1)\n",
        "#\n",
        "\n",
        "print(car_sales_for_sales.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qru4_cnjUJc8"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# Assignation of the one hot encoding to the dataframe\n",
        "car_sales_for_sales[manufacturer_one_hot.columns] = manufacturer_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIhliGYhVgLe"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# Check the first ten entries of the dataframe\n",
        "car_sales_for_sales.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TScGE77_VjdH"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# Remove the nans from the dataframe\n",
        "car_sales_ii_ready = car_sales_for_sales.dropna(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lATQyfjyV74U"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# Show the dataframe\n",
        "car_sales_ii_ready"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lgv_5dqvuYy"
      },
      "source": [
        "### Features/labels selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McTWTrDL4qUz"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# Total amount of columns\n",
        "print(f\"The list of columns is {car_sales_ii_ready.columns}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shhcXals5KXa"
      },
      "source": [
        "#### Ex 24: Features and labels selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaFBLXZGV9iP"
      },
      "outputs": [],
      "source": [
        "### Ex 24: Features and labels selection\n",
        "\n",
        "# Select Price, Engine size, horsepower, wheelbase, width, length, curb_weight\n",
        "# fuel capacity, fuel efficiency, power perf factor, size and all the brand\n",
        "# names as input variables.\n",
        "input_variables = []\n",
        "#\n",
        "\n",
        "# Select sales as output variable\n",
        "output_variable = ''\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQcjcaJeWgxz"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# Preparation of the data\n",
        "input_variables.append(output_variable)\n",
        "\n",
        "X_ii = car_sales_ii_ready[input_variables].dropna(axis=0)\n",
        "\n",
        "y_ii = X_ii[output_variable]\n",
        "X_ii.drop([output_variable], axis=1, inplace=True)\n",
        "\n",
        "print(f\"{len(X_ii.columns)} input variables: {X_ii.columns}\")\n",
        "print(f\"Output variable is: {y_ii.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVMwp-PDvzjG"
      },
      "source": [
        "### Normalization/standarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp9lVrMF6Gj9"
      },
      "source": [
        "#### Ex 25: Standarise the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj3kmBeaWkgU"
      },
      "outputs": [],
      "source": [
        "### Ex 25: Instantiate standaridisers\n",
        "# Modify the following line\n",
        "standard_x_ii = \n",
        "standard_y_ii = \n",
        "#\n",
        "standard_x_ii.fit(X_ii)\n",
        "standard_y_ii.fit(y_ii.to_numpy().reshape(-1,1))\n",
        "X_standard_ii = standard_x_ii.transform(X_ii)\n",
        "y_standard_ii = standard_y_ii.transform(y_ii.to_numpy().reshape(-1,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWJDXlWT6Ksx"
      },
      "source": [
        "### Tran/test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpUgcvDU6QFr"
      },
      "source": [
        "#### Ex 26: Train/test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83Yy-hKNWo5u"
      },
      "outputs": [],
      "source": [
        "### Ex 26: Split the dataset into train and test\n",
        "# Look for the function to split the dataset and use a test size of 0.3 and \n",
        "# random state 1.\n",
        "X_standard_train_ii, X_standard_test_ii, y_standard_train_ii, y_standard_test_ii = \\\n",
        "(X_standard_ii, y_standard_ii, ..., ...)\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz_rW2qCv7D-"
      },
      "source": [
        "### PCA\n",
        "So far in this problem, we have many features (inputs) but there are not many examples. PCA is a good method of reduce the dimensionality of a problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbXCj2YB8oc9"
      },
      "source": [
        "#### Ex 27: PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gakmYrC3d5Pt"
      },
      "outputs": [],
      "source": [
        "### Ex 27: Complete the PCA call to do a sweep with the number of components\n",
        "\n",
        "var_exp = []\n",
        "for i in range(1, len(input_variables)):\n",
        "  # Modify the following line\n",
        "  pca = PCA()\n",
        "  #\n",
        "  pca.fit(X_standard_train_ii)\n",
        "  var_exp.append(np.sum(pca.explained_variance_ratio_))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.arange(1, len(input_variables)), var_exp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiI4tmm_80-Z"
      },
      "source": [
        "#### Ex 28: Select PCA components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThUHMd-eWtUP"
      },
      "outputs": [],
      "source": [
        "### Ex 28: Select a suitable number of components\n",
        "\n",
        "#\n",
        "pca = PCA(n_components=...)\n",
        "#\n",
        "pca.fit(X_standard_train_ii)\n",
        "\n",
        "features_train = pca.transform(X_standard_train_ii)\n",
        "labels_train = y_standard_train_ii\n",
        "features_test = pca.transform(X_standard_test_ii)\n",
        "labels_test = y_standard_test_ii\n",
        "\n",
        "# features_train = X_standard_train_ii\n",
        "# labels_train = y_standard_train_ii\n",
        "# features_test = X_standard_test_ii\n",
        "# labels_test = y_standard_test_ii"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8W6TOrnv9WD"
      },
      "source": [
        "### Fitting models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjUsHdFoWwJm"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "reg, svm_linear, svm_poly, svm_rbf, clf = fit_models(features_train, labels_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMby-gHpwAKV"
      },
      "source": [
        "### Evaluation of models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pOzD1k0W0HC"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "methods_vector = [reg, svm_linear, svm_poly, svm_rbf, clf]\n",
        "methods_name = ['Linear Regressor', 'SVM Linear', 'SVM Poly', 'SVM RBF', 'Decision Tree']\n",
        "evaluation = evaluation_of_methods(methods_vector, methods_name, \n",
        "                                   features_train, features_test,\n",
        "                                   labels_train, labels_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AoGPbEpY7UH"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UXHCdyLW5z5"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "plotting_methods(methods_vector, features_test, labels_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3UnFzRFY1Ni"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "real_errors = standard_y_ii.inverse_transform((labels_test.ravel() - svm_poly.predict(features_test).ravel()).reshape(-1,1))\n",
        "print(f\"Real errors {real_errors.ravel()}\")\n",
        "print(f\"Average real error {np.mean(real_errors)}\")\n",
        "print(f\"Standar deviation real error {np.std(real_errors)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dPOgsJewEa-"
      },
      "source": [
        "## 6 - Conclusions and take aways\n",
        "1. Every data science project is different, requires different approach and methods to get to the objective successfully.\n",
        "2. First step should always be a \"Exploratory Data Analysis\" where the statistical distributions of the data are checked as well as the correlation between the variables.\n",
        "3. Unsupervised learning (targets/labels are not defined) is used to detect hidden patterns or relationships in the data. Commonly clustering, association and anomaly detection.\n",
        "4. Normalization/Standarization is most of the times required to bring all the data to the same scale, so models are not influence by the size of the data but for its importance.\n",
        "5. Supervised learning (features and targets are defined) is used to model some variables based on others."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
