{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Science for the Automotive Industry: 2nd and 3rd practical session - DL\n",
        "\n",
        "In this session, we will dive into deep learning. We will grasp the potential of neural networks in various applications increasing the level of complexity.\n",
        "\n",
        "1. Perceptron for regression on linear functions\n",
        "2. Perceptron for regression on non-linear Functions\n",
        "3. CNN for classification of images\n",
        "\n",
        "Developed by Nicolas Gutierrez."
      ],
      "metadata": {
        "id": "WgM4EVa42Qio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing required libraries\n",
        "It is a good practice loading the required libraries for the code at the start of it. Additionally, doing it this way you can have some hints about what the code below will do, just by checking the types of libraries imported."
      ],
      "metadata": {
        "id": "3V8yRZzYWmMS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo8-hRs62KMW"
      },
      "outputs": [],
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# File operations\n",
        "import glob\n",
        "import os\n",
        "# Numeric operations\n",
        "import numpy as np\n",
        "# Neural networks\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "# Libraries for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import time\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "# Libraries for pictures\n",
        "import PIL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Perceptron for regression on linear functions\n",
        "The easiest possible example of a neural network is using just one layer and try to learn a linear function. We will do that in this section step by step."
      ],
      "metadata": {
        "id": "uuoDSi3u8ybY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparation of the data"
      ],
      "metadata": {
        "id": "vKj8k04g9TXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ex 1: Creation of linear function"
      ],
      "metadata": {
        "id": "LHp-PTN--5HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 1: Create a linear function with slope and bias (intercept)\n",
        "\n",
        "def linear_function(x):\n",
        "  # Complete the following line with a linear function of x\n",
        "  y = \n",
        "  #\n",
        "  return y"
      ],
      "metadata": {
        "id": "bN5GF15BZH9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ex 2: Selection of train interval"
      ],
      "metadata": {
        "id": "V3M0Mu8o--3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 2: Select a training and test interval\n",
        "\n",
        "# Complete the following lines with either values or lists\n",
        "start_point_of_training = # Integer\n",
        "end_point_of_training = # Integer\n",
        "\n",
        "values_for_testing = [# list of values separated by commas]\n",
        "#\n",
        "\n",
        "x_train = np.linspace(start_point_of_training, end_point_of_training, 256).reshape(-1, 1)\n",
        "y_train = linear_function(x_train).reshape(-1, 1)\n",
        "\n",
        "x_test = np.array(values_for_testing).reshape(-1,1)\n",
        "y_test = linear_function(x_test).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "_9WAhOfw2E_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiation of the model and definition of the NN\n",
        "We will make use of [Tensorflow](https://www.tensorflow.org/) and [Keras](https://keras.io/) python packages.\n",
        "\n",
        "Keras is a high level API for creating NN, it can be seen as the front end of the NN creation. \n",
        "\n",
        "Tensorflow is the low level package running behind the scenes, it can be seen as the back end."
      ],
      "metadata": {
        "id": "3-MmGVVW9NgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ex 3: Instantiate a NN\n",
        "Keras has mainly [two ways of creating models](https://keras.io/api/models/):\n",
        "1. [Sequential](https://keras.io/guides/sequential_model/) -> Best to start.\n",
        "2. [Functional API](https://keras.io/guides/functional_api/) -> Best to extract the most out of Keras.\n",
        "\n",
        "As this is a introductory course we will use the Sequential API. In the exercise below, you will need to find how you instantiate a sequantial model from keras and add an input layer and a Dense layer."
      ],
      "metadata": {
        "id": "3sv0STE__GMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 3: Instantiate a 1 layer 1 neuron Neural Network\n",
        "\n",
        "# Intantiate a sequential model from keras\n",
        "model = keras.\n",
        "#\n",
        "\n",
        "# Add to the model an input layer of shape (1,) and a Dense layer with 1 neuron\n",
        "model.add()\n",
        "model.add()\n",
        "#"
      ],
      "metadata": {
        "id": "BI0xAT2VWtF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ex 4: Compile the NN\n",
        "Compilation configures and set ups the model for training. [Many parameters can be specified as this stage](https://keras.io/api/models/model_training_apis/), but the main ones are:\n",
        "*    loss -> Function that will compute the losses in every training step.\n",
        "*    optimizer -> The routine that will modify the weights of the NN to make it fit the data.\n",
        "*    metrics -> Metrics that will be calculated in every step so we see how the NN evolves throughout training."
      ],
      "metadata": {
        "id": "ZHuAFS5q_ROU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 4: Compile the NN\n",
        "\n",
        "# Compile the model using MeanAbsoluteError as loss, Adam as optimizer and \n",
        "# 'mean_squared_error', 'mean_absolute_error' as metrics\n",
        "model.compile(loss=,\n",
        "              optimizer=,\n",
        "              metrics=['', ''])\n",
        "#"
      ],
      "metadata": {
        "id": "b2-_-kzbdkAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitting"
      ],
      "metadata": {
        "id": "zDAewbXe9bHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ex 5: Fit the NN\n",
        "The fitting of NN is the process of adjusting the weights so the architecture develops the requested task."
      ],
      "metadata": {
        "id": "Tm3Vtfx7_Vhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 5: Fit the NN\n",
        "\n",
        "# Fit the model with x_train, y_train, a batch size of 8, 256 epochs, include \n",
        "# the validation data and verbose 1\n",
        "history = model.fit(, ,\n",
        "                    ,\n",
        "                    ,\n",
        "                    validation_data=(, ),\n",
        "                    )\n",
        "#\n",
        "\n",
        "models = []\n",
        "histories = []\n",
        "models.append(model)\n",
        "histories.append(history)"
      ],
      "metadata": {
        "id": "bhNyBx9reSbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "133XJY7t9pAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "# The following is an arbitrary academic function to check the outputs of \n",
        "# this case.\n",
        "\n",
        "def plotting_models(models, histories, x_train, y_train, x_test, y_test):\n",
        "  fig, ax = plt.subplots(ncols=2, figsize=np.array([2*6.4, 4.8]))\n",
        "\n",
        "  ax[0].scatter(x_test, y_test, label='Real values', zorder=1)\n",
        "  for i in range(len(models)):\n",
        "    ax[0].scatter(x_test, models[i].predict(x_test), label= f'Predictions_{i}', zorder=2)\n",
        "  ax[0].axvline(np.min(x_train), label='Train interval_min', color='red', zorder=0)\n",
        "  ax[0].axvline(np.max(x_train), label='Train interval_max', color='green', zorder=0)\n",
        "  ax[0].set_xlabel('X values')\n",
        "  ax[0].set_ylabel('Y Values')\n",
        "  ax[0].legend()\n",
        "  #plt.xlim(right=15)\n",
        "  \n",
        "  for i in range(len(histories)):\n",
        "    ax[1].plot(histories[i].history['mean_absolute_error'], label=f'MAE_{i}')\n",
        "    ax[1].plot(histories[i].history['val_mean_absolute_error'], label=f'MAE_val_{i}')\n",
        "  ax[1].set_ylabel('Mean Absolute Error')\n",
        "  ax[1].set_xlabel('epoch')\n",
        "  ax[1].legend()"
      ],
      "metadata": {
        "id": "V2kADWuajqYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "plotting_models(models, histories, x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "23WAsNBTwxxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ex 6: Plot model architecture ([ref](https://keras.io/api/utils/))"
      ],
      "metadata": {
        "id": "uZlcHBWhPQuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 6: Look for a keras utils function to plot the architecture of the model\n",
        "\n",
        "# Function to plot the architecture of the model\n",
        "(, '', show_shapes=True)\n",
        "#"
      ],
      "metadata": {
        "id": "-RtE-hyYjr__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Perceptron for regression on non-linear Functions\n",
        "We have seen NN acting with linear functions, they can do that, but the case where they are really strong is when non linearities come into play. In this section we will tackle a couple of examples."
      ],
      "metadata": {
        "id": "2wZM764Z90OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "# The following function is a custom callback to show the progress of the training\n",
        "# throughout the training process\n",
        "# More about CustomCallbacks here: https://keras.io/guides/writing_your_own_callbacks/\n",
        "\n",
        "class CustomCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, epoch_number, model, x_train, x_test, y_test):\n",
        "        self.epoch_number = epoch_number\n",
        "        self.model = model\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.epoch_number == 0:\n",
        "          y_pred = self.model.predict(self.x_test)\n",
        "          plt.cla()\n",
        "          pl.scatter(self.x_test, self.y_test, label='Real values', color=u'#1f77b4', zorder=1)\n",
        "          pl.scatter(self.x_test, y_pred, label= 'Predictions', color= u'#ff7f0e', zorder=2)\n",
        "          pl.axvline(np.min(x_train), label='Train interval_min', color='red', zorder=0)\n",
        "          pl.axvline(np.max(x_train), label='Train interval_max', color='green', zorder=0)\n",
        "          pl.xlabel('X values')\n",
        "          pl.ylabel('Y Values')\n",
        "          pl.title(f\"Epoch {epoch}\")\n",
        "          # pl.legend()\n",
        "          display.display(pl.gcf())\n",
        "          display.clear_output(wait=True)\n",
        "          time.sleep(0.05)"
      ],
      "metadata": {
        "id": "0iGnjX4kbclk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Piecewise functions\n",
        "A [piecewise function](https://en.wikipedia.org/wiki/Piecewise) is a function defined by cases or splitted into several functions. "
      ],
      "metadata": {
        "id": "91qYzaYY-dAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparation of the data"
      ],
      "metadata": {
        "id": "iHdMK3fxkX_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ex 7: Define a Piecewise"
      ],
      "metadata": {
        "id": "h00Mi5cV_eTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 7: Define a piecewise function using np.piecewise\n",
        "\n",
        "def piecewise_function(x):\n",
        "  # Define a piecewise function that is continuous in the interval -10 to 10\n",
        "  y = \n",
        "  #\n",
        "  return y"
      ],
      "metadata": {
        "id": "TuktvrwI-fNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "x_tosee = np.linspace(-10, 10, 256)\n",
        "y_tosee = piecewise_function(x_tosee)\n",
        "plt.scatter(x_tosee, y_tosee)\n",
        "plt.xlabel('X values')\n",
        "plt.ylabel('Y values')\n",
        "plt.title('Piecewise function')"
      ],
      "metadata": {
        "id": "pOOzrCF7g9wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# The reshapes here are required a vector of components into a vector of vectors with one component\n",
        "x_train = np.linspace(-10, 10, 256).reshape(-1, 1)\n",
        "y_train = piecewise_function(x_train).reshape(-1, 1)\n",
        "\n",
        "x_test = np.linspace(-50, 50, 256).reshape(-1,1)\n",
        "y_test = piecewise_function(x_test).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "EwqymU2IAdbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fitting"
      ],
      "metadata": {
        "id": "sfUgwuBTAeNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ex 7: Complete compile and fit function"
      ],
      "metadata": {
        "id": "pFmRTise_ihZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 8: Complete the following function\n",
        "# NOTE: This function is not required for your future work, it is an arbitrary \n",
        "# function for academic purposes. It just handy having it this way so you can \n",
        "# experiment with the main parameters of a Perceptron.\n",
        "\n",
        "def compile_and_fit_nonlinear(neurons, epochs, lr, bs, x_train, y_train, x_test, y_test, minval, maxval):\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  # Exercise 8.1: Instantiate a sequential model and add an input layer of shape (1,)\n",
        "  model = \n",
        "  model.\n",
        "  #\n",
        "\n",
        "  for i in range(len(neurons)):\n",
        "    initializer = tf.keras.initializers.RandomUniform(minval=minval, \n",
        "                                                      maxval=maxval, \n",
        "                                                      seed=1)\n",
        "    # Exercise 8.2:\n",
        "    # Add a Dense layer to the model with \"relu\" activation, \n",
        "    # kernel_initializer = initializer and bias_initializer as Ones. The number\n",
        "    # of neurons will be neurons[i]\n",
        "    model.\n",
        "    #\n",
        "  \n",
        "  # Exercise 8.3:\n",
        "  # Add a Dense layer to the model with one neuron\n",
        "  model.\n",
        "  #\n",
        "\n",
        "  # Exercise 8.4:\n",
        "  # Compile the model using MeanSquaredError as loss, Adam with learning_rate = lr \n",
        "  # as optimizer and 'mean_squared_error', 'mean_absolute_error' as metrics\n",
        "  model.\n",
        "  #\n",
        "  \n",
        "  # Exercise 8.5:\n",
        "  # Fit the model using the following options batch size = bs, epochs=epochs, True for shuffle,\n",
        "  # use training data and validation data. use as a callback the class defined\n",
        "  # previously with inputs (100, model, x_train, x_test, y_test)\n",
        "  history = model.(, ,\n",
        "                      ,\n",
        "                      ,\n",
        "                      ,\n",
        "                      validation_data=(, ),\n",
        "                      ,\n",
        "                      callbacks=[CustomCallback(100, model, x_train, x_test, y_test)]\n",
        "                      )\n",
        "  #\n",
        "\n",
        "  model.summary()\n",
        "  for i in range(len(model.layers)):\n",
        "    print(f\"Layer: {i}\")\n",
        "    print(model.layers[i].weights[0].numpy())\n",
        "    print(model.layers[i].bias.numpy())\n",
        "  return model, history"
      ],
      "metadata": {
        "id": "EMPgh0PK2TQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ex 9: Modify NN architecture\n",
        "Neural networks are very Ad-Hoc models that need to be adjusted for every problem. Normally there is no one architecture fits all. In this exercise you will need to change the most common parameters in an NN:\n",
        "*    __neurons__ -> Amount of neurons per layer. The more neurons the wider the NN. This is normally related with the amount of input data and how you want to modify the information throughout your network.\n",
        "*    __layers__ -> Amount layers of the model. The more layers, the higher complexity and level of non-linearities your NN can tackle succesfully. Very deep NN can have issues for coverging. \n",
        "*    __learning_rate__ -> this is related with how much you allow the optimizer changing parameters throughout the training process. High rates mean faster convergence but sub-optimal final result (or even no convergence at all).\n",
        "*    __batch_size__ -> This is the amount of data the NN sees before changing the weights. Normally the higher the better, but it is usually limited by the amount of memory available.\n"
      ],
      "metadata": {
        "id": "NqspXUiA_vAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 9: Play with the values in this cell to see how they affect the results\n",
        "\n",
        "models = []\n",
        "histories = []\n",
        "\n",
        "# Modify the next lines\n",
        "neurons = [] # Every entry in the list adds a new layer\n",
        "epochs = \n",
        "lr = \n",
        "batch_size = \n",
        "min_val_for_random_init = \n",
        "max_val_for_random_init = \n",
        "#\n",
        "\n",
        "model, history = compile_and_fit_nonlinear(neurons, epochs, lr, batch_size, \n",
        "                                           x_train, y_train, x_test, y_test, \n",
        "                                           min_val_for_random_init, max_val_for_random_init)\n",
        "models.append(model)\n",
        "histories.append(history)\n",
        "\n",
        "# test_scores = model.evaluate(x_test, y_test, verbose=2)"
      ],
      "metadata": {
        "id": "jQqmzE3mAfyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation"
      ],
      "metadata": {
        "id": "e3H4rUKCArid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "plotting_models(models, histories, x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "TMGzB5kzUvSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trigonometric functions"
      ],
      "metadata": {
        "id": "dKmn3rQVlUWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparation of the data"
      ],
      "metadata": {
        "id": "rDlbYAcwlYXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ex 10: Define a trigonometric"
      ],
      "metadata": {
        "id": "CNuHUIg0_2xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 10: Define e trigonometric function of x using numpy\n",
        "\n",
        "def trigonometric(x):\n",
        "  # Use numpy to define a trigonometric function of x\n",
        "  y = \n",
        "  #\n",
        "  return y"
      ],
      "metadata": {
        "id": "PJwA1rXIldVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# Plot to doublecheck\n",
        "x_tosee = np.linspace(0, 2*np.pi, 256)\n",
        "y_tosee = trigonometric(x_tosee)\n",
        "plt.scatter(x_tosee, y_tosee)\n",
        "plt.xlabel('X values')\n",
        "plt.ylabel('Y values')\n",
        "plt.title('Trigonometric function')"
      ],
      "metadata": {
        "id": "fz3V67Y5kMx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "x_train = np.linspace(0, 2*np.pi, 256).reshape(-1, 1)\n",
        "y_train = trigonometric(x_train).reshape(-1, 1)\n",
        "\n",
        "x_test = np.linspace(-3*np.pi, 3*np.pi, 512).reshape(-1,1)\n",
        "y_test = trigonometric(x_test).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "d6waeFyznjkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fitting"
      ],
      "metadata": {
        "id": "Chn8yQKBlcrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Ex 11: Compile and Fit"
      ],
      "metadata": {
        "id": "X9PMW-4rACYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 11: Use the function compile_and_fit_non_linear\n",
        "\n",
        "models = []\n",
        "histories = []\n",
        "\n",
        "# Modify the next lines\n",
        "# Use the suggested function with at least three layers with less than 10 neurons\n",
        "# thousands of epochs, lr 0.005 and bs multiple of 2 up to 256, minval -0.5 y maxval 0.5\n",
        "neurons = [, , ] # Every entry in the list adds a new layer\n",
        "epochs = \n",
        "lr = \n",
        "batch_size = \n",
        "min_val_for_random_init = \n",
        "max_val_for_random_init = \n",
        "#\n",
        "\n",
        "model, history = compile_and_fit_nonlinear(neurons, epochs, lr, batch_size, \n",
        "                                           x_train, y_train, x_test, y_test, \n",
        "                                           min_val_for_random_init, max_val_for_random_init)\n",
        "models.append(model)\n",
        "histories.append(history)\n",
        "\n",
        "# test_scores = model.evaluate(x_test, y_test, verbose=2)"
      ],
      "metadata": {
        "id": "h8l4atlDnOCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation"
      ],
      "metadata": {
        "id": "imrIMa55lfsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "plotting_models(models, histories, x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "aLad3LUWsjVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - CNN for classification of images\n",
        "The first exercise we will do here is developing a NN as a classifier between cars and non cars pictures. For that, we will have available a subset of pictures from the following references:\n",
        "\n",
        "- [Car or Not a Car](https://medium.com/@oviyum/lessons-from-fine-tuning-a-convolutional-binary-classifier-ccf9388e46d8)\n",
        "- [Cars Dataset](https://ai.stanford.edu/~jkrause/cars/car_dataset.html)\n",
        "- [Caltech256](http://www.vision.caltech.edu/Image_Datasets/Caltech256/)"
      ],
      "metadata": {
        "id": "ZF2S29vhs2xX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparation of the data"
      ],
      "metadata": {
        "id": "MA-_viUS4WTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# Properties of the model\n",
        "img_height = 128\n",
        "img_width = 128\n",
        "batch_size = 256"
      ],
      "metadata": {
        "id": "1FK6C6MwZZkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# You will receive a prompt asking for permissions to access your google drive \n",
        "# from google collab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Qa5q07Y0s52I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ex 12: Find the data in your drive"
      ],
      "metadata": {
        "id": "WPLyFlVzLoNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 12: Look for the cars and non cars folder\n",
        "\n",
        "# Look for the following folders in your google drive\n",
        "cars_training_folder = '/content/drive/MyDrive/...'\n",
        "cars_test_folder = '/content/drive/MyDrive/...'\n",
        "noncars_train_folder = '/content/drive/MyDrive/...'\n",
        "noncars_test_folder = '/content/drive/MyDrive/...'\n",
        "#\n",
        "\n",
        "list_of_files = glob.glob(cars_training_folder)\n",
        "print(\"\\nCar train folder:\")\n",
        "print(list_of_files[:2])\n",
        "\n",
        "list_of_files = glob.glob(cars_test_folder)\n",
        "print(\"\\nCar test folder:\")\n",
        "print(list_of_files[:2])\n",
        "\n",
        "list_of_files = glob.glob(noncars_train_folder)\n",
        "print(\"\\nNon-Car train folder:\")\n",
        "print(list_of_files[:2])\n",
        "\n",
        "list_of_files = glob.glob(noncars_test_folder)\n",
        "print(\"\\nNon-Car test folder:\")\n",
        "print(list_of_files[:2])"
      ],
      "metadata": {
        "id": "hovbkQi9IEhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ex 13: Display a picture"
      ],
      "metadata": {
        "id": "GkKGakmwLtuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "def show_example(folder_path):\n",
        "  list_of_pictures = glob.glob(folder_path + \"*.jpg\")\n",
        "  number_of_pictures = len(list_of_pictures)\n",
        "  random_picture = list_of_pictures[np.random.randint(0, high=number_of_pictures-1)]\n",
        "  print(f\"Example file: {random_picture}\")\n",
        "  print(f\"Number of jpg files: {number_of_pictures}\")\n",
        "  return str(random_picture)"
      ],
      "metadata": {
        "id": "3syfK7CK02f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 13: Use the function show_example to verify you have selected the folders correctly\n",
        "\n",
        "# Include your line here\n",
        "random_picture = \n",
        "#\n",
        "\n",
        "PIL.Image.open(random_picture)"
      ],
      "metadata": {
        "id": "szHScSLg1fgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ex 14: Keras image dataset"
      ],
      "metadata": {
        "id": "W6MK6MhGL2nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 14: Look for the correct folders to use image_dataset_from_directory from keras\n",
        "\n",
        "# Complete the path as a string below\n",
        "data_dir_train = '/content/drive/MyDrive/...'\n",
        "#\n",
        "\n",
        "# Train dataset\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir_train,\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size,\n",
        "  label_mode='binary')\n",
        "\n",
        "# Complete the path as a string below\n",
        "data_dir_test = '/content/drive/MyDrive/...'\n",
        "#\n",
        "\n",
        "# Test dataset\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir_test,\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size,\n",
        "  label_mode='binary')\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(f\"Training classes: {class_names}\")\n",
        "\n",
        "class_names = test_ds.class_names\n",
        "print(f\"Test classes: {class_names}\")\n",
        "\n",
        "# The output of this cell should be:\n",
        "# Found 1000 files belonging to 2 classes.\n",
        "# Found 300 files belonging to 2 classes.\n",
        "# Training classes: ['cars', 'others']\n",
        "# Test classes: ['cars', 'others']"
      ],
      "metadata": {
        "id": "nAf0v-o54PNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "# Extra check, if you run this cell you will a 3 by 3 set of pictures with the corresponding labels\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[int(labels[i])])\n",
        "    plt.axis(\"off\")\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "1cyX31Lg5wWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitting with convolutions"
      ],
      "metadata": {
        "id": "AX1rh0kfNjo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ex 15: Compile and fit function"
      ],
      "metadata": {
        "id": "_Bo13YTwM9rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 15: Complete the following function to instantiate, compile and fit a convolutional neural network\n",
        "\n",
        "def compile_and_fit_conv(convfilters, convsize, densesize, lr, epochs, train_ds, val_ds):\n",
        "  # Sanity checks\n",
        "  if len(convfilters) != len(convsize):\n",
        "    raise IndexError('Length of convfilters and convsize is required to be the same.')\n",
        "  if len(convfilters) < 1 or len(convsize) < 1:\n",
        "    raise IndexError('Length of convfilters or convsize is required to be higher than 0.')\n",
        "  if len(densesize) < 1:\n",
        "    raise IndexError('Length of densesize is required to be higher than 0.')\n",
        "\n",
        "  # Gettting some handy variables\n",
        "  class_names = train_ds.class_names\n",
        "  num_classes = len(class_names)\n",
        "\n",
        "  # Cleaning keras backend to avoid piling up training phases\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  # Definition of the model\n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)))\n",
        "  for i in range(len(convfilters)):\n",
        "    initializer = tf.keras.initializers.HeNormal(seed=1)\n",
        "    # Exercise 13.1:\n",
        "    # Add to the model a suitable convolutional layer, use padding 'same' and activation 'relu'\n",
        "    model.,\n",
        "    #\n",
        "    \n",
        "    # Exercise 13.2:\n",
        "    # Add to the model a suitable max pooling layer\n",
        "    model.,\n",
        "    #\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "\n",
        "  for j in range(len(densesize)):\n",
        "    # Exercise 13.3:\n",
        "    # Add to the model a Dense layer, use activation \"relu\" and initializer as a kernel initializer\n",
        "     model.\n",
        "    #\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  # Exercise 13.4\n",
        "  # Copile the model using a loss function as binary crossentropy (logits=False),\n",
        "  # Adam with lr as learning rate and accuracy as metrics\n",
        "  model.\n",
        "  #\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  # Exercise 13.5\n",
        "  # Fit the model\n",
        "  history = model.\n",
        "  #\n",
        "  \n",
        "  return model, history"
      ],
      "metadata": {
        "id": "Zr5oIbsJ6TVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ex 16: Select hyperparameters"
      ],
      "metadata": {
        "id": "uthMHxCuNYIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 16: Select suitable parameters for compile and fit function\n",
        "\n",
        "# Do not modify the format of the variables\n",
        "convfilters = [, , ]\n",
        "convsize = [, , ]\n",
        "densesize = []\n",
        "lr =  # Careful with this value, around 0.01 should be fine\n",
        "epochs =  # Around 20 should be fine\n",
        "#\n",
        "\n",
        "model, history = compile_and_fit_conv(convfilters, convsize, densesize, lr, epochs, train_ds, test_ds)"
      ],
      "metadata": {
        "id": "lh2GtptV_RC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation with convolutions"
      ],
      "metadata": {
        "id": "qLeupmW6Nyxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "def plotting_prediction(history):\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  \n",
        "  fig, ax = plt.subplots(ncols=2, figsize=np.array([2*6.4, 4.8]))\n",
        "\n",
        "  ax[0].plot(np.arange(1,len(acc)+1), acc, label='Training Accuracy')\n",
        "  ax[0].plot(np.arange(1,len(acc)+1), val_acc, label='Validation Accuracy')\n",
        "  ax[0].legend()\n",
        "  ax[0].set_title('Training and Validation Accuracy')\n",
        "  ax[0].set_xlabel('Epochs')\n",
        "  ax[0].set_ylabel('Accuracy')\n",
        "  \n",
        "  ax[1].plot(np.arange(1,len(loss)+1), loss, label='Training Loss')\n",
        "  ax[1].plot(np.arange(1,len(loss)+1), val_loss, label='Validation Loss')\n",
        "  ax[1].legend()\n",
        "  ax[1].set_title('Training and Validation Loss')\n",
        "  ax[1].set_xlabel('Epochs')\n",
        "  ax[1].set_ylabel('Loss')"
      ],
      "metadata": {
        "id": "YbW0ND8dN1Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "plotting_prediction(history)"
      ],
      "metadata": {
        "id": "C0pwbDOsO6Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ex 17: Locate the validation pictures"
      ],
      "metadata": {
        "id": "2yMXMvW8Nkc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 17: Locate the validation pictures\n",
        "\n",
        "# Modify the following line\n",
        "test_pictures_path = '/content/drive/MyDrive/...'\n",
        "#\n",
        "\n",
        "sample_pictures = glob.glob(test_pictures_path)\n",
        "print(sample_pictures)\n",
        "\n",
        "# The output of this cell should show 3 pictures: \n",
        "# \"validation_01.jpg\", \"validation_02.jpg\", \"validation_03.jpg\""
      ],
      "metadata": {
        "id": "wIFsLMed_hLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "def test_other_pictures(list_of_pictures):\n",
        "  for i in range(len(list_of_pictures)):\n",
        "    print(f\"Picture number {i}: {os.path.basename(list_of_pictures[i])}\")\n",
        "    test_picture = np.array(PIL.Image.open(list_of_pictures[i]))\n",
        "    print(f\"Picture size {test_picture.shape}\")\n",
        "    test_picture_resized = tf.expand_dims(tf.image.resize(test_picture, (img_height, img_width)),0)\n",
        "    print(f\"Piture size after resize {test_picture_resized.shape}\")\n",
        "    prediction = model.predict(test_picture_resized)\n",
        "    if prediction > 0.5: \n",
        "      prediction_class = 1\n",
        "    else:\n",
        "      prediction_class = 0\n",
        "    print(f\"Model prediction: {prediction} - {class_names[prediction_class]}\\n\")"
      ],
      "metadata": {
        "id": "FF7aQjVfGNA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "test_other_pictures(sample_pictures)"
      ],
      "metadata": {
        "id": "XTB8nqXhGn3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ex 18: Save the NN"
      ],
      "metadata": {
        "id": "xMKfmn5uNo6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 18: Store the model in your google drive for later use\n",
        "\n",
        "# Modify the following line to include the corresponding method\n",
        "model.('.h5')\n",
        "#"
      ],
      "metadata": {
        "id": "6Mylk-DSGt0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ex 19: Load the NN and Test it"
      ],
      "metadata": {
        "id": "0FnteZJtNum3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Exercise 19: Load the model and check the results are the same\n",
        "\n",
        "# Delete the model first\n",
        "del model\n",
        "#\n",
        "\n",
        "# Then load it as 'model' from the file you have created previously\n",
        "model = ('.h5')\n",
        "#"
      ],
      "metadata": {
        "id": "S-F3vsfEV3SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Do not modify this cell, not an exercise\n",
        "\n",
        "test_other_pictures(sample_pictures)"
      ],
      "metadata": {
        "id": "wC9yCbx8WAcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 - Conclusions and take aways\n",
        "1. Neural networks (NN) are very powerful and specially tailored for very non linear problems.\n",
        "2. An NN can potentially map any input to output, given a good architecture and enough relevant data.\n",
        "3. We have seen two classic architectures: Perceptron and Convolutional Neural Networks (CNN)\n",
        "4. Perceptrons are made of dense layers.\n",
        "5. CNN are typically made of blocks with 2D Convolutions and Max Pooling 2D.\n",
        "\n",
        "To learn more:\n",
        "*   [Dense NN](https://analyticsindiamag.com/a-complete-understanding-of-dense-layers-in-neural-networks/#:~:text=Terms%20and%20Conditions.-,What%20is%20a%20Dense%20Layer%3F,in%20artificial%20neural%20network%20networks.)\n",
        "*   [A Comprehensive Guide to Convolutional Neural Networks](https://saturncloud.io/blog/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way/)\n",
        "*   [Keras API documentation](https://keras.io/api/)\n",
        "*   [Deep Learning Specialization (Coursera)](https://www.coursera.org/specializations/deep-learning?utm_medium=sem&utm_source=gg&utm_campaign=B2C_EMEA_deep-learning_deeplearning-ai_FTCOF_specializations_country-UK-country-GB&campaignid=19970507700&adgroupid=154882314224&device=c&keyword=andrew%20ng%20deep%20learning%20course&matchtype=b&network=g&devicemodel=&adposition=&creativeid=654977645500&hide_mobile_promo&gclid=Cj0KCQjwlumhBhClARIsABO6p-wSSTunFeuKJKc_T5jTtXL0jOogeYybLqr-wB7u_kqLwzsWlh7WFO4aAjr0EALw_wcB)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xjtTrzxzW4Uv"
      }
    }
  ]
}